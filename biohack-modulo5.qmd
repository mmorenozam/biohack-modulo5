---
logo: images/icon1_nb.png
lang: es
filters:
  - timer
format: 
  letterbox-revealjs:
    theme: custom.scss
    progress: false
    menu: true
    width: 1400
    height: 720
    slide-number: true
    preview-links: auto
    touch: true
    chalkboard:
      theme: whiteboard
      boardmarker-width: 4
      buttons: false
    revealjs-plugins:
      - pointer
      
callout-appearance: simple
---

## 

::: columns
::: {.column width="37.5%"}
![](images/icon1_nb.png){style="margin-left:-60px"}
:::

::: {.column width="60%"}
::: {.title data-id="title"}
M√≥dulo 5: Introducci√≥n a la Estad√≠stica Inferencial con R
:::

Mauricio Moreno, PhD
:::
:::

![](images/cover.png){.image-left}

## Generalidades {.smaller}

::: incremental
-   Uno de los objetivos de este curso es el evitar en la medida de lo posible el adentrarnos en teor√≠a estad√≠stica. Entre los temas que dejaremos de lado est√°n:

    -   Teor√≠a de la probabilidad b√°sica

    -   Ley de los n√∫meros grandes

    -   Distribuci√≥n muestral (o de muestreo)

    -   Teorema del l√≠mite central
    
    -   Valores cr√≠ticos de una prueba

    -   Descripci√≥n a detalle de la distribuci√≥n normal

    -   Descripci√≥n a detalle de otras distribuciones

-   Para qui√©n est√© interesado en un recurso para ver estos temas a profundidad, recomiendo el libro de [Danielle Navarro: *Learning Statistics with R*](https://learningstatisticswithr.com/){target="_blank"}

:::

<!-- ## Pasos b√°sicos en un protocolo estad√≠stico -->

<!-- De manera muy general: -->

<!-- ::: incremental -->

<!-- 1.    Determinar el tipo de estudio a realizar -->

<!-- 2.    Establecer una hip√≥tesis -->

<!-- 3.    Listar m√©todos o modelos estad√≠sticos para analizar los datos -->

<!-- 4.    Decidir sobre el dise√±o experimental a implementar y sus requerimientos -->

<!-- ::: -->

# Definiciones b√°sicas

## Muestras, poblaciones y muestreos

::: incremental
-   **Muestra**: Es un conjunto de observaciones que provienen de una poblaci√≥n de inter√©s. Idealmente, esta deber√≠a ser lo suficientemente grande para hacer inferencias de esa poblaci√≥n.

-   **Poblaci√≥n**: Es el conjunto de todas las posibles observaciones de las que tengamos inter√©s en realizar inferencias. Es vital el definir adecuadamente sus caracter√≠sticas.

-   **Muestreo**: Es el proceso por el cual obtendremos nuestra muestra para un estudio. En estudios experimentales, el muestreo se entiende tambi√©n como el proceso de aleatorizaci√≥n/randomizaci√≥n de unidades experimentales.
:::

## Muestreo simple sin reemplazo

![Tomado de [*Learning Statistics with R*](https://learningstatisticswithr.com/){target="_blank"}](images/simple1.png){fig-align="center" width="900"}

## Muestreo simple con reemplazo

![Tomado de [*Learning Statistics with R*](https://learningstatisticswithr.com/){target="_blank"}](images/simple2.png){fig-align="center" width="1020"}

## Otros tipos de muestreo

::: incremental
-   **Muestreo sistem√°tico**: consiste en tomar un determinado elemento de la poblaci√≥n siguiendo un patr√≥n. Por ejemplo, escoger los m√∫ltiplos de cuatro enumerados en una lista de posibles individuos de estudio (sol√≠a ser una pr√°ctica com√∫n en ensayos cl√≠nicos).

-   **Muestreo a conveniencia**: consiste en incluir en el estudio a todos los elementos disponibles de la poblaci√≥n de inter√©s. Esto sucede sobre todo con poblaciones escasas o de dificil acceso (ejemplo, realizar estudios en comunidades LGBTIQ+).

-   **Muestreo estratificado**: es una combinaci√≥n del muestreo simple con los sujetos agrupados por alguna caracter√≠stica en com√∫n, por ejemplo sexo, edad, h√°bitat (suele ser usado en exit polls y conteos r√°pidos).
:::

## Par√°metros poblacionales y estad√≠sticos muestrales

::: incremental

-   Par√°metros poblacionales: llamados tambi√©n verdaderos. Corresponden al escenario en que todos los individuos de una poblaci√≥n podr√≠an ser medidos con respecto a una caracter√≠stica.
    
-   Estad√≠sticos muestrales: son aproximaciones de los par√°metros verdaderos que corresponden a mediciones de una muestra de la poblaci√≥n.

-   La estad√≠stica gira alrededor de la inferencia sobre estad√≠sticos muestrales, ya que en el pr√°ctica, los par√°metros poblacionales son pr√°cticamente imposibles de determinar.

:::

## Media aritm√©tica {.smaller}

| S√≠mbolo        | ¬øQu√© es?                                        | ¬øSabemos qu√© es?              |
|----------|------------------------------------|----------------------|
| $\overline{X}$ | Media aritm√©tica de la muestra                  | Calculada de los datos        |
| $\mu$          | Verdadera media aritm√©tica de la poblaci√≥n      | Casi nunca es conocida        |
| $\hat{\mu}$    | Estimado de la media aritm√©tica de la poblaci√≥n | S√≠, identica a $\overline{X}$ |

$$
\overline{X} = \frac{1}{n}\sum^{n}_{i=1}\left(X_i\right)
$$

## Desviaci√≥n est√°ndar {.smaller}


| S√≠mbolo        | ¬øQu√© es?                                          | ¬øSabemos qu√© es?           |
|---------|---------------------------------------|---------------------|
| $s$            | Desviaci√≥n est√°ndar de la muestra                 | Calculada de los datos     |
| $\sigma$       | Verdadera desviaci√≥n est√°ndar de la poblaci√≥n     | Casi nunca es conocida     |
| $\hat{\sigma}$ | Estimado de la deviaci√≥n est√°ndar de la poblaci√≥n | S√≠, pero no es igual a $s$ |

::: columns
::: {.column width="50%"}
$$
s = \sqrt{\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2} 
$$
:::

::: {.column width="50%"}
$$
\sigma = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2} 
$$
:::
:::

## Varianza {.smaller}


| S√≠mbolo          | ¬øQu√© es?                                | ¬øSabemos qu√© es?             |
|----------|-------------------------------|------------------------|
| $s^2$            | Varianza de la muestra                  | Calculada de los datos       |
| $\sigma^2$       | Verdadera varianza de la poblaci√≥n      | Casi nunca es conocida       |
| $\hat{\sigma}^2$ | Estimado de la varianza de la poblaci√≥n | S√≠, pero no es igual a $s^2$ |

::: columns
::: {.column width="50%"}
$$
s^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2
$$
:::

::: {.column width="50%"}
$$
\sigma^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 
$$
:::
:::

## Intervalos de confianza {.smaller}

::: incremental
-   Los estimados de las verdaderas $\mu$ y $\sigma$ ($\hat{\mu}$ y $\hat{\sigma}$) provienen de distribuciones de muestreo, y como tales, inherentemente poseen cierto grado de incertidumbre.

-   Los intervalos de confianza son medidas que nos permiten tener una idea de esa incertidumbre.

-   En el estudio de la distribuci√≥n normal est√°ndar tenemos el conocimiento que existe un 95% de chances que una cantidad normalmente distribuida colectada al azar, estar√° distante de la media aritm√©tica entre $\pm$ 1.96 desviaciones est√°ndar.
:::

. . .

$$
\overline{X} - \left(1.96\times\frac{\sigma}{\sqrt{n}}\right) \leq \mu \leq \overline{X} + \left(1.96\times\frac{\sigma}{\sqrt{n}}\right)
$$

::: incremental
-   Y se interpreta como: **con un 95% de confianza, podemos esperar que la media aritm√©tica verdadera de la poblaci√≥n de inter√©s se encuentra contenida entre...**
:::

. . .

$$
\text{IC}_{95}=\overline{X} \pm \left(1.96\times\frac{\sigma}{\sqrt{n}}\right) 
$$

## Intervalos de confianza {.smaller visibility="uncounted"}

::: incremental
-   Sin embargo, como mencionamos $\sigma$ es casi nunca conocido, y es necesario hacer una correcci√≥n a la f√≥rmula anterior. La distribuci√≥n normal trabaja bien baja la presunci√≥n de un n√∫mero grande de observaciones.

-   En su lugar, en 1908 el estad√≠stico [Gosset](https://en.wikipedia.org/wiki/Student%27s_t-distribution){target="_blank"} parametriz√≥ una distribuci√≥n para muestras peque√±as que asemeja a la normal. Con el tiempo, esta distribuci√≥n adopt√≥ el nombre de *Student*.

-   Y es precisamente que la f√≥rmula anterior es corregida con la distribuci√≥n de Student y as√≠ poder calcular intervalos de confianza para muestras peque√±as usando $s$ en lugar de $\sigma$:
:::

. . .

$$
\text{IC}_{95}=\overline{X} \pm \left(t_{n-1,\alpha/2}\times\frac{s}{\sqrt{n}}\right) 
$$

::: incremental
-   Donde el valor $t_{n-1,\alpha/2}$ refiere a:

    -   $n-1$: los grados de libertad, igual al n√∫mero de observaciones $n$ de la muestra, menos 1

    -   $\alpha$: es el nivel de significancia (probabilidad de obtener un resultado err√≥neo por azar).

    -   Estos valores en el pasado se encontraban tabulados en libros de texto, hoy contamos con R!
:::

## Ejemplo de par√°metros poblacionales y estad√≠sticos muestrales

::: incremental
-   Supongamos que el IQ de toda una poblaci√≥n puede estar caracterizado por una media aritm√©tica, $\mu$, igual a 100, con una desviaci√≥n est√°ndar, $\sigma$, igual a 15.

-   Si tomo una muestra de 100 individuos de dicha poblaci√≥n, podr√≠a tener una media aritm√©tica de esta muestra, $\overline{X}$, igual a 101.4 y una desviaci√≥n est√°ndar de la muestra, $s$, igual a 13.7.

-   As√≠ $\overline{X}$ y $s$ son aproximaciones a los valores verdareros de $\mu$ y $\sigma$ de esa poblaci√≥n.
:::

## Ejemplo de par√°metros poblacionales y estad√≠sticos muestrales {.smaller visibility="uncounted"}

::: incremental
-   Supongamos que en lugar de tener acceso al IQ de 100 personas, medimos al azar el IQ de s√≥lamente 5 personas y deseamos calcular el $\text{IC}_{95}$
:::

. . .

```{r echo=T, eval=T, error=T}
#| code-line-numbers: "1|2|3|4|5|6|7"
IQ_muestra <- c(101, 98, 116, 96, 129)   # muestra
n <- 5                                   # n√∫mero de observaciones
t95 <- qt(p = 0.975, df = n -1)          # valor de Student para 4 grados de libertad al 5%
x <- mean(IQ_muestra)                    # media aritm√©tica de la muestra
s <- sd(IQ_muestra)                      # desviaci√≥n est√°ndar de la muestra
ls <- x + (t95*s/(n-1))                  # l√≠mite superior del IC95
li <- x - (t95*s/(n-1))                  # l√≠mite inferior del IC95
```

## Intervalos de confianza {autoanimate="true" visibility="uncounted" .smaller}

-   Supongamos que en lugar de tener acceso al IQ de 100 personas, medimos al azar el IQ de s√≥lamente 5 personas y deseamos calcular el $\text{IC}_{95}$

```{r echo=T, eval=T, error=T}
#| code-line-numbers: "8"
IQ_muestra <- c(101, 98, 116, 96, 129)   # muestra
n <- 5                                   # n√∫mero de observaciones
t95 <- qt(p = 0.975, df = n -1)          # valor de Student para 4 grados de libertad al 5%
x <- mean(IQ_muestra)                    # media aritm√©tica de la muestra
s <- sd(IQ_muestra)                      # desviaci√≥n est√°ndar de la muestra
ls <- x + (t95*s/(n-1))                  # l√≠mite superior del IC95
li <- x - (t95*s/(n-1))                  # l√≠mite inferior del IC95
print(paste0("Con un 95% de confianza podemos esperar que la verdadera media aritm√©tica de IQ de esta poblaci√≥n se encuentre entre [",round(li,0),", ",round(ls,0),"]"))
```

## Hip√≥tesis de investigaci√≥n vs. hip√≥tesis estad√≠sticas {.smaller}

::: incremental
-   Una hip√≥tesis de investigaci√≥n gira alrededor del desarrollar una conclusi√≥n cient√≠fica acerca de un tema de inter√©s del investigador. Ejemplos: *el fumar causa c√°ncer*, *las vacunas causan/previenen enfermedades*.

    -   Es decir, pueden tener una naturaleza subjetiva, que expresan la pregunta del investigador de una manera general sin mayor descripci√≥n del ¬øc√≥mo? voy a probar o descartarla, ni ¬øen qu√© extensi√≥n?.

-   Hip√≥tesis estad√≠sticas, por el contrario, deben ser matem√°ticamente precisas y basadas en las caracter√≠sticas de los datos que recolectemos con el fin de probar o descartar la hip√≥tesis de investigaci√≥n.

    -   C√≥mo es de esperar, el probar o descartar una hip√≥tesis estad√≠stica ser√° √∫nicamente v√°lida para la poblaci√≥n sobre la cual una muestra fue tomada.

    -   Es ah√≠ donde radica la importancia en definir la poblaci√≥n sujeto de estudio de manera planificada con el objetivo de que cumpla tantos detalles sean necesarios de la hip√≥tesis de investigaci√≥n. Ejemplo, el modelo animal m√°s usado es el rat√≥n. Si bien es cierto constituye uno de los primeros pasos en el desarrollo de muchas investigaciones, los hallazgos en ratones NO pueden ser inmediatamente atribuibles a suceder en seres humanos.
:::

## Hip√≥tesis nula y alternativa {.smaller}

::: incremental
-   La formulaci√≥n de hip√≥tesis estad√≠sticas puede reducirse a establer preguntas de investigaci√≥n en forma de las hip√≥tesis nula y alternativa.

-   Supongamos que tenemos dos grupos experimentales para probar la eficiencia de un nuevo procedimiento quir√∫rgico. Un grupo de pacientes ser√° sometido a la intervenci√≥n tradicional (control), y el otro grupo al nuevo procedimiento (experimental).

    -   La hip√≥tesis nula ($H_0$) establece que: no existe diferencia entre el grupo control y el grupo experimental,

    -   Mientras que la hip√≥tesis alternativa ($H_a$) establece que: s√≠ existe differencia entre ambos.
:::

. . .

```{=tex}
\begin{align}
H_0& : \mu_c = \mu_e& H_0& : \mu_c- \mu_e =0 \\
H_a& : \mu_c \neq \mu_e& H_a& : \mu_c- \mu_e \neq 0
\end{align}
```
## Tipos de errores

::: incremental
-   Al llevar a cabo pruebas de hip√≥tesis pueden ocurrir errores
:::

. . .

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(xtable)
```

```{r, results='asis'}
dat <- data.frame(
  " " = c("H_{0}\\text{ es verdadera}", "H_{0}\\text{ es falsa}"),
  "\\text{Acepta }H_{0}" = c("\\text{Desici√≥n correcta}", "\\text{Error tipo II}"),
  "\\text{Rechaza }H_{0}" = c("\\text{Error tipo I}", "\\text{Desici√≥n correcta}"),
  check.names = FALSE
)

M <- print(xtable(dat, align=rep("|c|", ncol(dat)+1)), 
           floating = FALSE, tabular.environment="array", 
           comment=FALSE, print.results=FALSE, 
           include.rownames = FALSE,
           sanitize.text.function = function(x) x)
cat(M)
```

. . .

-   ¬øDe qu√© depende que aceptemos correctamente o no la hip√≥tesis nula?

. . .

Las pruebas estad√≠sticas dependen de la cantidad de variaci√≥n y la diferencia entre tratamientos a detectar (**tama√±o del efecto**). La soluci√≥n: aumentar el n√∫mero de observaciones

## Poder de una prueba estad√≠stica {.smaller}

::: incremental
-   El poder de una prueba estad√≠stica es la probabilidad de rechazar la hip√≥tesis nula cuando esta es de hecho falsa.

-   Se puede derivar de la tabla anterior
:::

. . .

```{r, results='asis'}
dat <- data.frame(
  " " = c("H_{0}\\text{ es verdadera}", "H_{0}\\text{ es falsa}"),
  "\\text{Acepta }H_{0}" = c("1-\\alpha\\text{ (Prob. decisi√≥n correcta)}", "\\beta\\text{ (Taza Error tipo II)}"),
  "\\text{Rechaza }H_{0}" = c("\\alpha\\text{ (Taza Error tipo I)}", "1-\\beta\\text{ (Poder)}"),
  check.names = FALSE
)

M <- print(xtable(dat, align=rep("|c|", ncol(dat)+1)), 
           floating = FALSE, tabular.environment="array", 
           comment=FALSE, print.results=FALSE, 
           include.rownames = FALSE,
           sanitize.text.function = function(x) x)
cat(M)
```

. . .

-   En la pr√°ctica, existen f√≥rmulas cerradas para la determinaci√≥n del n√∫mero m√≠nimo de observaciones para alcanzar un poder adecuado ($\ge$ 80%)

-   A este procedimiento se le conoce como an√°lisis de poder o determinaci√≥n del tama√±o de la muestra y del que hablaremos en m√°s detalle m√°s adelante.

## Tama√±o del efecto {.smaller}

::: incremental
-   El tama√±o del efecto ($\theta$) es un valor que por lo general es determinado por el investigador y que puede ser la diferencia de inter√©s a detectar en una prueba estad√≠stica.

-   Por ejemplo, supongamos que tenemos un grupo de ratones que poseen una media de 110 mg/dL de glucosa en sangre y podr√≠an ser parte de un linaje para ser usado como modelo para hiperglucemia. Si el valor normal de glucosa en ratones es de 100 mg/dL, el investigador estar√≠a interesado en saber cu√°l es el n√∫mero de ratones necesarios para con un 80\% de poder, determinar si el tama√±o de efecto $\theta = 10$ es estad√≠sticamente distinto de 0.

-   Las hip√≥tesis de esta prueba se ver√≠a as√≠
:::

. . .

```{=tex}
\begin{align}
H_0& : \mu_c- \mu_r = \theta \\
H_a& : \mu_c- \mu_r \neq \theta
\end{align}
```

. . .

-   Esta hip√≥tesis corresponde a la prueba estad√≠stica m√°s com√∫n: *dos colas*

## Pruebas de dos colas

```{=tex}
\begin{align}
H_0& : \mu_c- \mu_r = \theta \\
H_a& : \mu_c- \mu_r \neq \theta
\end{align}
```
![Imagen tomada de [*UCLA: Advanced Research Computing*](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/){target="_blank"}](images/dcolas.gif){fig-align="center" width="400"}

## Pruebas de una cola

```{=tex}
\begin{align}
H_0& : \mu_c- \mu_r \ge \theta \\
H_a& : \mu_c- \mu_r < \theta
\end{align}
```
![Imagen tomada de [*UCLA: Advanced Research Computing*](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/){target="_blank"}](images/ucolagreat.gif){fig-align="center" width="400"}

## Pruebas de una cola {visibility="uncounted"}

```{=tex}
\begin{align}
H_0& : \mu_c- \mu_r \le \theta \\
H_a& : \mu_c- \mu_r > \theta
\end{align}
```
![Imagen tomada de [*UCLA: Advanced Research Computing*](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/){target="_blank"}](images/ucolaless.gif){fig-align="center" width="400"}

## El valor p {.smaller}

::: columns
::: {.column width="40%"}

::: incremental
-   Pero ¬øc√≥mo sabemos si una hip√≥tesis es aceptada o rechazada?

-   El valor p, describe que tan probable ser√≠a observar resultados de la prueba asumiendo que la hip√≥tesis nula no hubiese sido rechazada. Por ello, a menores valores p, mayor la diferencia estad√≠stica con respecto a la hip√≥tesis alternativa.

-   El valor p tambi√©n depende de que tan "estricta" queramos sea la prueba, y esto se define con el grado de significancia (usualmente igual a 5\%, y a 1\%)
:::
:::

::: {.column width="60%"}


![](images/pval.png){fig-align="center"}
:::
:::

:::footer
Imagen tomada de [*Towards DataScience*](https://towardsdatascience.com/what-is-p-value-370056b8244d){target="_blank"}
:::

## Antes de continuar

![](images/cena.png){fig-align="center"}

::: footer
Imagen tomada de [aqu√≠](https://arbor-analytics.com/post/2022-10-10-p-ing-in-the-woods-p-values-in-forest-science/){target="_blank"}
:::

## Antes de continuar {.smaller visibility="uncounted"}

::: incremental
-   El umbral de 0.05 es una convenci√≥n arbitraria [creada por Fischer](https://www.bmj.com/rapid-response/2011/11/03/origin-5-p-value-threshold#:~:text=statistician%20RA%20Fisher%20in%201926%20%5B1%5D.){target="_blank"} en los inicios de la estad√≠stica moderna.

-   Lastimosamente, se ha generalizado la idea de que por m√°s m√≠nima sea la diferencia con respecto a 0.05, esta representa la diferencia entre publicar o no (en el campo acad√©mico), entre lanzar o no un nuevo f√°rmaco/producto al mercado (en la industria).

-   En 2014, debido a un fallo de la [corte suprema de justicia de los Estados Unidos](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017929/#:~:text=A%20statistically%20significant%20test%20result,that%20no%20effect%20was%20observed.){target="_blank"} que le dio la potestad a los inversionistas de farmace√∫ticas a demandarlas por fallar en reportar efectos secundarios de sus productos a pesar de haber sido hallados estad√≠sticamente no significativos, la Asociaci√≥n Americana de Estad√≠stica (ASA) se vio en la necesidad de definir m√°s exhaustivamente el concepto del valor p.

-   Entre las recomendaciones de la ASA, se enfatiz√≥ el dar mayor prioridad a la estimaci√≥n de otros estad√≠sticos complementarios al valor p, tales como intervalos de confianza u otros provenientes de la estad√≠stica Bayesiana (intervalos de credibilidad, factores de Bayes).

-   Esta √∫ltima (estad√≠stica Bayesiana), ofrece una interpretaci√≥n m√°s natural al referirnos a los resultados en t√©rminos de probabilidades y no en n√∫meros arbirtrarios como el valor p.

-   En resumen, una investigaci√≥n no es in√∫til si el valor p sobrepasa o est√° por debajo de 0.05 por cantidades peque√±as.
:::

## Antes de continuar {.smaller .smaller visibility="uncounted"}

::: incremental
-   En su lugar, en escenarios en que el valor p est√° alejado por una d√©cima o varias cent√©simas de 0.05, los resultados deber√≠an interpretarse como indeterminados para generalizar sobre la poblaci√≥n objeto de estudio y espec√≠ficos a las condiciones experimentales (an√°lisis estad√≠sticos, instrumentos de medici√≥n, etc) bajo las cuales fueron tomadas y modeladas las mediciones.

-   En el contexto de los modelos estad√≠sticos que veremos m√°s adelante, esto ha derivado en un "temor" del investigador cuando los resultados no pasan los chequeos de los supuestos sobre los que estos modelos se cimentan. Sobre todo cuando el valor p dista de 0.05 por √≠nfimas cantidades.

-   Esto puede llevar a malas pr√°cticas cient√≠ficas tales como: no reportar el resultado de los chequeos, blindar los datos, escoger "outliers" y removerlos y en el peor de los casos, manipular los datos para tratar de acomodar nuestros datos a estos chequeos.

-   Todo lo que he mencionado, no solamente constituyen casos de mala conducta cient√≠fica, sino lo que hoy en d√≠a se le conoce como *p hacking* (que se puede resumir a torturar los datos hasta que nos confiesen una verdad agradable a nuestros prop√≥sitos).
:::

# Pruebas estad√≠sticas param√©tricas

## Pruebas t {.smaller}

::: incremental
-   Las pruebas t son usadas para encontrar la diferencia entre dos medias aritm√©ticas.

-   La $H_0$ en estas pruebas es que las medias aritm√©ticas son las mismas.

-   Se rechaza la $H_0$ cuando el valor p resultante es $<$ 0.05

-   Existen tres tipos de pruebas t

    -   Pruebas t de una muestra

    -   Pruebas t de muestras independientes

    -   Pruebas t de muestras emparejadas

-   Estas pruebas fueron desarrolladas bajo la suposici√≥n de la **normalidad** y de **homogeneidad de las varianzas**.

-   De acuerdo al teorema del l√≠mite central, muestras grandes casi aseguran la normalidad.

-   Cuando el n√∫mero de observaciones en una muestra es peque√±o, es recomendable llevar a cabo un test de normalidad para decidir si es posible una prueba t o una de sus alternativas.
:::

## Normalidad de una muestra

::: incremental
-   Antes de llevar a cabo las pruebas t, hemos mencionado sus supuestos. Por ello, es aconsejable el siempre realizar estas pruebas antes de usarlas.

-   Existen dos tipos de pruebas para establecer si una muestra es normalmente distribuida o no

    -   Indirectamente: gr√°fico Q-Q

    -   Prueba formal de normalidad (ejemplo: Shapiro-Wilk)

-   **En el caso del ANOVA**, es importante enfatizar que estas pruebas no necesariamente tienen que hacerse antes de la prueba, como ya veremos m√°s adelante.
:::

## Gr√°fico Q-Q

::: incremental
-   El gr√°fico Q-Q es una prueba visual indirecta de la normalidad.

-   Consiste en crear un gr√°fico de dispersi√≥n entre los valores observados de una muestra vs. los valores que deber√≠an estos tener si siguieran una distribuci√≥n normal.

-   Mientras en el gr√°fico de dispersi√≥n los puntos m√°s se distribuyan a lo largo de una diagonal, m√°s cercanos est√°n los datos de la muestra a seguir un distribuci√≥n normal.

-   Su desventaja es que es muy subjetivo, y a menudo requiere una prueba formal para poder confirmarlo.
:::

## Gr√°fico Q-Q {visibility="uncounted"}

```{r echo=T, eval=F, error=T, fig.width=6, fig.align='center'}
#| code-line-numbers: "1|2|3"
set.seed(123)
y <- rnorm(n = 30, mean = 0, sd = 1)  # simulamos 30 observaciones de una normal estandar
qqnorm(y)                             # producimos el gr√°fico Q-Q
```

## Gr√°fico Q-Q {visibility="uncounted"}

```{r echo=T, eval=T, error=T, fig.width=6, fig.align='center'}
set.seed(123)
y <- rnorm(n = 30, mean = 0, sd = 1)  # simulamos 30 observaciones de una normal estandar
qqnorm(y)                             # producimos el gr√°fico Q-Q
```

## Prueba de normalidad Shapiro-Wilk

::: incremental
-   La $H_0$ de esta prueba (y del resto de pruebas formales de normalidad) es que un set de $n$ observaciones es normalmente distribuido.

-   Otro conocido m√©todo es Kolmogorov-Smirnov. Sin embargo, Shapiro-Wilk es m√°s apropiado para cuando el n√∫mero de muestras es menor a 50.

-   Para ilustrar su uso, chequeemos la normalidad de los datos que simulamos anteriormente
:::

. . .

```{r echo=T, eval=T, error=T, fig.width=6, fig.align='center'}
shapiro.test(y)
```

## Prueba de homogeneidad de las varianzas {.smaller}

::: incremental
-   En el caso de comparaciones entre las medias de dos grupos, la homogeneidad de varianzas puede chequearse usando la prueba F.

-   La prueba t de una muestra no requiere chequear este supuesto.

-   Para ilustrar su uso, creemos otro vector con datos simulados. En este caso, un igual n√∫mero de observaciones con la misma desviaci√≥n est√°ndar pero diferente media:
:::

. . .

```{r echo=T, eval=T, error=T, fig.width=6, fig.align='center'}
set.seed(123)
x <- rnorm(n = 30, mean = 4, sd = 1)
var.test(x, y)
```

## Supuestos en la pr√°ctica {.smaller}

```{r echo=T, eval=F, error=T}
install.packages("UsingR")
library(UsingR)
data(crime)
shapiro.test(crime$y1983)
shapiro.test(crime$y1993)
var.test(crime$y1983, crime$y1993)
```


. . .

```{r echo=F, eval=T, error=T, message=F, warning=F}
library(UsingR)
```

::: columns
::: {.column .fragment width="50%"}
Normalidad 1983

```{r echo=T, eval=T, error=T}
shapiro.test(crime$y1983)     
```
:::

::: {.column .fragment width="50%"}
Normalidad 1993

```{r echo=T, eval=T, error=T}
shapiro.test(crime$y1993)     
```
:::
:::

. . .

::: columns
::: {.column width="50%"}

Homogeneidad de las varianzas

```{r echo=T, eval=T, error=T, fig.width=6, fig.align='center'}
var.test(crime$y1983, crime$y1993)
```

:::

::: {.column width="50%}

![](images/calmarno.jpg){fig-align="right" width=300}

:::

:::


## Transformaci√≥n de variables

::: incremental
-   A menudo nos encontraremos con conjuntos de observaciones que no cumplen uno o ninguno de los supuestos.

-   Antes de considerar pruebas no param√©tricas, podemos intentar transformaciones de variables para regresar al mundo de las pruebas param√©tricas. Las transformaciones m√°s usadas son:

    -   La ra√≠z cuadrada (si los datos no contienen n√∫meros negativos)

    -   Elevar al cuadrado

    -   Logaritmo (si los datos no contienen n√∫meros negativos)
:::

## Transformaci√≥n de variables {visibility="\"uncounted"}

::: incremental
-   Existe un m√©todo m√°s sofisticado para "normalizar" una muestra. [[La transformaci√≥n de Box-Cox.](https://www.r-bloggers.com/2022/10/box-cox-transformation-in-r/){target="_blank"}]{.fragment}

-   Cuando se trabaja con muestras transformadas, el objetivo es poder revertir la transformaci√≥n a las unidades reales para as√≠ poder hacer conclusiones sobre las inferencias estad√≠sticas.

-   En otras palabras, una misma transformaci√≥n **debe** aplicarse a todos los grupos a ser comparados. **NO** tiene ning√∫n sentido tratar de realizar inferencias entre grupos donde se hayan usado distintas transformaciones para normalizarlos.

-   Si el n√∫mero de observaciones es muy reducido, usualmente no hay transformaci√≥n que funcione y se recomienda usar directamente pruebas no param√©tricas.
:::

## Transformaci√≥n de variables {.scrollable visibility="uncounted" .smaller}

::: columns
::: {.column .fragment width="50%"}
Ra√≠z cuadrada

```{r echo=T, eval=T, error=T}
shapiro.test(sqrt(crime$y1983))     
```
:::

::: {.column .fragment width="50%"}
Elevar al cuadrado

```{r echo=T, eval=T, error=T}
shapiro.test(crime$y1983^2)     
```
:::
:::

::: columns
::: {.column .fragment width="50%"}
Logaritmo

```{r echo=T, eval=T, error=T}
shapiro.test(log(crime$y1983))     
```
:::

::: {.column .fragment width="50%"}
Chequeemos con el otro grupo

```{r echo=T, eval=T, error=T}
shapiro.test(log(crime$y1993))     
```
:::
:::

. . .

Homogeneidad de las varianzas con transformaci√≥n logar√≠tmica

```{r echo=T, eval=T, error=T, fig.width=6, fig.align='center'}
var.test(log(crime$y1983), log(crime$y1993))
```

## Prueba t de una muestra

::: incremental
-   Es usada para comparar la media aritm√©tica de una muestra con un valor conocido (un est√°ndar por ejemplo).

-   Por lo general el valor al que se va a comparar proviene de referencias bibliogr√°ficas, pre-experimentos o supociones fundamentadas.

-   En este caso, el supuesto que debe cumplirse es el de la normalidad de los datos

-   Regresando al ejemplo de los ratones, determinemos si la media de la siguiente muestra es mayor al l√≠mite superior de glucosa de ratones saludables (100 mg/dL).
:::

## Prueba t de una muestra {visibility="uncounted" autoanimate="true"}

```{r echo=T, eval=F, error=T}
#| code-line-numbers: "1|2|3|4"
glc_rat <- c(108.7, 93.7, 52.7, 79.0, 74.7, 131.9, 99.5, 63.3, 98.6, 92.7)
ratest <- t.test(glc_rat,
       mu = 100,
       alternative = "greater")
ratest
```

## Prueba t de una muestra {visibility="uncounted" autoanimate="true"}

```{r echo=T, eval=T, error=T}
glc_rat <- c(108.7, 93.7, 52.7, 79.0, 74.7, 131.9, 99.5, 63.3, 98.6, 92.7)
ratest <- t.test(glc_rat,
       mu = 100,
       alternative = "greater")
ratest
```

## Prueba t de muestras independientes

::: incremental
-   Es usada para comparar las medias aritm√©ticas de dos grupos independientes.

-   Por ejemplo, si deseas comparar las medias aritm√©ticas de individuos agrupados por sexo.

-   Para ilustrar esta prueba, vamos a hacer uso de la tabla de datos de `genderweight` del paquete `{datarium}`.

    -   Veamos si existe una diferencia significativa en la media del peso entre hombres y mujeres
:::

. . .

```{r echo=F, eval=T, error=T}
library(datarium)
```

```{r echo=T, eval=T, error=T}
install.packages("datarium")
library(datarium)
data(genderweight)
```

## Prueba t de muestras independientes {.smaller visibility="uncounted"}

::: columns
::: {.column .fragment width="50%"}
Chequeamos normalidad: Group M

```{r echo=T, eval=T, error=T}
shapiro.test(subset(genderweight, group == "M")$weight)     
```
:::

::: {.column .fragment width="50%"}
Chequeamos normalidad: Group F

```{r echo=T, eval=T, error=T}
shapiro.test(subset(genderweight, group == "F")$weight)     
```
:::
:::

. . .

Chequeamos la homogeneidad de las varianzas

```{r echo=T, eval=T, error=T}
var.test(genderweight$weight ~ genderweight$group)
```

::: incremental
-   **¬°La homogeneidad de las varianzas no se cumple!** üò±
:::

## Prueba t de muestras independientes {visibility="uncounted" .smaller}

::: incremental

-   ¬øDebemos transformar? [No necesariamente]{.fragment}

-   El no cumplir con el supuesto de la homogeneidad de varianzas no es un gran problema gracias a varias correcciones.

-   La funci√≥n base de R `t.test` cuenta con el argumento `var.equal = F` como default.

-   Bajo este argumento, no se asumen varianzas iguales entre los grupos y en su lugar R lleva a cabo la aproximaci√≥n de Welch para lidiar con este problema.

:::

. . .

```{r echo=T, eval=T, error=T}
gendertest <- t.test(genderweight$weight ~ genderweight$group)
gendertest
```

## Prueba t para muestras emparejadas

::: incremental
-   Es usada para comparar las medias de dos grupos que guardan una relaci√≥n.

-   Esto solo ocurre cuando las medidas se han realizado en las mismas unidades experimentales. Ejemplos: individuos al inicio y al final de un tratamiento, muestras en una misma locaci√≥n geogr√°fica. 

-   Para esta prueba, vamos a usar la tabla de datos `crime` del paquete `{UsingR}`

    -   Veamos si existe una diferencia en las tasas de crimen (# de reportes/100000 habitantes) en 50 estados de los Estados unidos entre 1983 y 1993
:::

. . .

```{r echo=T, eval=T, error=T}
data(crime)
```

## Prueba t para muestras emparejadas {visibility="uncounted" autoanimate="true"}

```{r echo=T, eval=F, error=T}
#| code-line-numbers: "1"

crimetest <- t.test(x = log(crime$y1983), y = log(crime$y1993), paired = TRUE)
crimetest
exp(mean(log(crime$y1983)))
exp(mean(log(crime$y1993)))
```

## Prueba t para muestras emparejadas {visibility="uncounted" autoanimate="true"}

```{r echo=T, eval=T, error=T}
crimetest <- t.test(x = log(crime$y1983), y = log(crime$y1993), paired = TRUE)
crimetest
exp(mean(log(crime$y1983)))
exp(mean(log(crime$y1993)))
```

# Pruebas estad√≠sticas no param√©tricas

## Pruebas de Wilcoxon para datos no normales

::: incremental
-   Las pruebas de Wilcoxon usan la mediana como criterio para evaluar la $H_0$.

-   Lastimosamente, estas pruebas son usualmente menos poderosas (mayor tasa de errores tipo II).

-   Tiene dos formas:

    -   Pruebas para una muestra (an√°loga a la prueba t para una muestra)

    -   Pruebas para dos muestras (an√°loga a las pruebas t para dos muestras independientes y emparejadas)
:::

## Prueba de Wilcoxon para una muestra

::: incremental
-   [Prof. Danielle Navarro](https://learningstatisticswithr.com/){target="_blank"} midi√≥ el nivel de felicidad de sus estudiantes antes y despu√©s de su clase de Estad√≠stica. Ella estaba interesada en saber si el tomar una clase de Estad√≠stica tiene alg√∫n efecto en la felicidad de sus estudiantes. Los datos que obtuvo no est√°n normalmente distribuidos. Por ello, se vio en la necesidad de llevar a cabo una prueba de Wilcoxon.

-   En este caso, la $H_0$, es que la diferencia de la mediana de la felicidad de sus estudiantes antes y despu√©s de la clase deber√≠a ser igual a cero para concluir que no existe tal efecto.
:::

## Prueba de Wilcoxon para una muestra {visibility="uncounted"}

```{r echo=T, eval=T, error=T}
# Primero recreo la tabla de Prof. Navarro
felicidad <- data.frame(before = c(30,43,21,24,23,40,29,56,38,16),
                        after = c(6,29,11,31,17,2,31,21,8,21))
felicidad$change <- felicidad$after - felicidad$before

muestra_wilcox <- wilcox.test(felicidad$change, mu = 0)
muestra_wilcox
```

## Prueba de Wilcoxon para dos muestras

::: incremental
-   Regresando al ejemplo de la tabla de datos `genderweight`, supongamos que estos no est√°n normalmente distribuidos.

-   Usaremos la prueba de Wilcoxon para muestras independientes para ver si existe diferencia entre los pesos de hombres y mujeres.
:::

. . .

```{r echo=T, eval=T, error=T}
dos_wilcox <- wilcox.test(genderweight$weight ~ genderweight$group)
dos_wilcox
```

## `{report}` {.smaller}

::: incremental

-   El redactar resultados estad√≠sticos puede resultar en una tarea compleja.

-   Afortunadamente contamos con la ayuda de `{report}`, parte del multiverso `{easystats}` que nos puede dar gu√≠as para ello.

:::

. . .

```{r echo=T, eval=F, error=T}
install.packages("easystats")
library(report)
```

```{r echo=F, eval=T, error=T}
library(report)
```

```{r echo=T, eval=T, error=T}
report(dos_wilcox)
```

```{r echo=T, eval=T, error=T}
report(crimetest)
```


# An√°lisis de Varianza (ANOVA)

## Introducci√≥n

::: incremental
-   Hasta el momento nos hemos enfocado a los casos donde comparamos las medias entre dos grupos.

-   Pero es m√°s com√∫n el evaluar distintos tratamientos al mismo tiempo.

-   Para ello, contamos con el ANOVA, desarrollado por el estad√≠stico Ronald Fisher a inicios del siglo 20, y que sin duda es el m√©todo estad√≠stico m√°s usado hoy en d√≠a.

-   El objetivo de un ANOVA es el de determinar la existencia de diferencias entre las medias aritm√©ticas de las muestras representativas de $n$ poblaciones (o en t√©rminos m√°s precisos, tratamientos).
:::

## Supuestos del ANOVA {.smaller}

::: incremental
1.  Independencia de los datos: correcta randomizaci√≥n en el dise√±o del experimento.

2.  Homogeneidad de las varianzas: la varianza entre los tratamientos es la misma.

3.  **Normalidad**: [pero, ¬øde qu√© exactamente?]{.fragment}

-   C√≥mo vimos antes, la normalidad es un requisito para conducir pruebas t, y lo es tambi√©n para el ANOVA.

-   Muchos libros de texto y otros recursos, mencionan que los datos de cada tratamiento deben ser normalmente distribuidos para llevar a cabo un ANOVA. [Esto es cierto e impr√°ctico a la vez.]{.fragment}

-   Es com√∫n el sugerir el llevar a cabo una prueba de normalidad antes de un ANOVA, pero debemos considerar que:

    -   Cada tratamiento tiene su propia media, en caso de medias muy distantes entre s√≠, la prueba puede fallar.

    -   En su lugar, podr√≠as correr una prueba por cada tratamiento. Esto solo funciona con un considerable n√∫mero de observaciones/tratamiento.
:::

## Supuestos del ANOVA {.smaller visibility="uncounted"}

::: incremental
-   Esto nos puede llevar a soluciones err√≥neas como transformar datos, borrar outliers o utilizar pruebas no param√©tricas innecesariamente.

-   Entonces, ¬ø**normalidad** de qu√©?

-   De los residuos estandarizados!... [¬øQu√© es un residual?]{.fragment}

    -   Un residual es la diferencia entre una observaci√≥n y su predicci√≥n

    -   Un residual estandarizado resulta de la divisi√≥n del residual para la ra√≠z cuadrada de la predicci√≥n

    -   La distribuci√≥n muestral de los residuos estandarizados tiene media 0 y desviaci√≥n est√°ndar 1

-   Pero, ¬øpor qu√© la confusi√≥n? [Solo cuando el n√∫mero de observaciones es lo suficientemente grande, se tiene la certeza que los residuos ser√°n normalmente distribuidos.]{.fragment}

-   En resumen, es mejor chequear la normalidad despu√©s que realizamos el ANOVA.
:::


## ANOVA de una v√≠a {.smaller}

-   ANOVA de una v√≠a se refiere cuando tenemos m√°s de dos tratamientos que est√°n definidos por un solo factor a la vez.

-   Para esta secci√≥n usaremos el dataset de moscas de la fruta de la librer√≠a `{Stat2Data}`:

```{r echo=F, eval=T, error=T}
library(Stat2Data)
data(FruitFlies)
```

```{r echo=T, eval=F, error=T}
install.packages("Stat2Data")
library(Stat2Data)
data(FruitFlies)
```

::: incremental

-   Estos datos corresponden a un estudio realizado por Partridge y Farquhar reportados por [Hanley y Shapiro](https://www.tandfonline.com/doi/full/10.1080/10691898.1994.11910467){target="_blank"} acerca del comportamiento sexual de moscas de la fruta.

-   La pregunta de investigaci√≥n consisti√≥ en determinar si un incremento en la actividad sexual de moscas de sexo masculino reduce su esperanza de vida.


-   El experimento consisti√≥ de 125 moscas de sexo masculino que fueron asignadas al azar a los siguientes grupos:

    -   *8 v√≠rgenes*: un macho con 8 hembras v√≠rgenes
    
    -   *1 virgen*: un macho con una hembra virgen
    
    -   *8 embarazadas*: un macho con 8 hembras embarazadas
    
    -   *1 embarazada*: un macho con una hembra embarazada
    
    -   *ninguna*: un macho solo
:::

## ANOVA de una v√≠a {visibility="uncounted"}

::: columns
::: {.column width="50%"}


```{r echo=T, eval=F, error=T, fig.align = 'center'}
library(ggplot2)
FruitFlies |>
  ggplot(aes(x = Treatment, 
             y = Longevity, 
             group = Treatment))+
  geom_violin(fill = "blue", 
              alpha = 0.25)+
  geom_boxplot(width = 0.15,
               fill = "orange",
               outlier.color = "red",
               outlier.size = 3)+
  theme_bw()
```
:::

::: {.column  width="50%"}


```{r echo=F, eval=T, error=T, fig.align = 'center'}
library(ggplot2)
FruitFlies |>
  ggplot(aes(x = Treatment, 
             y = Longevity, 
             group = Treatment))+
  geom_violin(fill = "blue", 
              alpha = 0.25)+
  geom_boxplot(width = 0.15,
               fill = "orange",
               outlier.color = "red",
               outlier.size = 3)+
  theme_bw()
```
:::
:::

## ANOVA de una v√≠a en R

::: incremental
-   Existen dos formas de llevar a cabo ANOVA en R:

    1.  Crear un modelo lineal con la funci√≥n `lm` y luego el ANOVA con la funci√≥n `anova` sobre el objeto producto de `lm`.

    2.  Aplicar directamente la funci√≥n `aov` sobre nuestros datos.

-   Ambas funciones (`lm` y `aov`) tienen la misma sintaxis. [La primera opci√≥n es la m√°s usada.]{.fragment}

-   Adicionalmente, el paquete `car` ofrece la funci√≥n `Anova`. El resultado de ambas es pr√°cticamente el mismo para la mayor√≠a de modelos. [La ventaja de esta √∫ltima yace en que `Anova` puede realizar c√°lculos de los valores cr√≠ticos usando sumas de cuadrados de tipo I, II y III.]{.fragment}
:::

## ANOVA de una v√≠a en R

::: columns
::: {.column .fragment width="50%"}
Opci√≥n 1

```{r echo=T, eval=F, error=T, fig.align = 'center'}
#| code-line-numbers: "1|2|3|4"
library(car)
lm1 <- lm(Longevity ~ Treatment, data = FruitFlies)
Anova(lm1)
```
:::

::: {.column .fragment width="50%"}
Opci√≥n 2

```{r echo=T, eval=F, error=T, fig.align = 'center'}
#| code-line-numbers: "1|2|3"
anova1 <- aov(Longevity ~ Treatment, data = FruitFlies)
summary(anova1)
```
:::
:::

::: columns
::: {.column .fragment width="50%"}
```{r echo=F, eval=T, error=T, fig.align = 'center'}
library(car)
lm1 <- lm(Longevity ~ Treatment, data = FruitFlies)
Anova(lm1)
```
:::

::: {.column .fragment width="50%"}
```{r echo=F, eval=T, error=T, fig.align = 'center'}
anova1 <- aov(Longevity ~ Treatment, data = FruitFlies)
summary(anova1)
```
:::
:::

## ANOVA de una v√≠a en R {visibility="uncounted"}

::: columns
::: {.column .fragment width="50%"}
Opci√≥n 1

```{r echo=T, eval=F, error=T, fig.align = 'center'}
report(Anova(lm1))
```
:::

::: {.column .fragment width="50%"}
Opci√≥n 2

```{r echo=T, eval=F, error=T, fig.align = 'center'}
report(anova1)
```
:::
:::

::: columns
::: {.column .fragment width="50%"}
```{r echo=F, eval=T, error=T, fig.align = 'center'}
report(Anova(lm1))
```
:::

::: {.column .fragment width="50%"}
```{r echo=F, eval=T, error=T, fig.align = 'center'}
report(anova1)
```
:::
:::

## ANOVA de una v√≠a en R {visibility="uncounted"}

```{r echo=T, eval=T, error=T, fig.align = 'center'}
#| code-line-numbers: "1-2|3-6"
library(insight)
library(flextable)
anova1 |> 
  report_table() |> 
  format_table() |>
  flextable()
```

## Diagn√≥sticos del ANOVA

::: incremental
-   Antes de conducir pruebas formales para los supuestos del ANOVA, es preciso darle un vistazo a diagn√≥sticos visuales que podemos obtener del mismo.

-   El ANOVA es un caso de regresi√≥n lineal (con predictores categ√≥ricos), por lo que en esta secci√≥n nos centraremos en la interpretaci√≥n de estos diagn√≥sticos desde la perspectiva del ANOVA.

-   En el apartado de regresi√≥n lineal volveremos a profundizar en las interpretaciones de los mismos para ese caso determinado.

-   Para acceder a estos diagn√≥sticos, basta usar la funci√≥n `plot` sobre el objeto donde guardamos los resultados del modelo `lm1`.
:::

## Diagn√≥sticos del ANOVA {visibility="uncounted"}

```{r echo=T, eval=F, error=T, fig.align = 'center', fig.width=5}
lm1 <- lm(Longevity ~ Treatment, data = FruitFlies)
par(mfrow = c(2, 2))
plot(lm1)
par(mfrow = c(1, 1))
```

## Diagn√≥sticos del ANOVA {visibility="uncounted"}

```{r echo=F, eval=T, error=T, fig.align = 'center', fig.width=6}
par(mfrow = c(2, 2))
plot(lm1)
par(mfrow = c(1, 1))
```

## Diagn√≥sticos del ANOVA {.smaller visibility="uncounted"}

::: {.columns .v-center-container}
::: {.column  width="40%"}
```{r echo=F, eval=T, error=T, fig.height=6, fig.width=6}
plot(lm1, 1)
```
:::

::: {.column  width="60%"}
**Residuos vs. Valores ajustados**

En este plot podemos evidenciar departuras del supuesto de la homocedasticidad. Idealmente, la l√≠nea roja que se muestra deber√≠a ir a lo largo de la horizontal en la coordenada cero del eje y (sobre la l√≠nea entrecortada).
:::
:::

## Diagn√≥sticos del ANOVA {.smaller visibility="uncounted"}

::: {.columns .v-center-container}
::: {.column  width="40%"}
```{r echo=F, eval=T, error=T, fig.height=6, fig.width=6}
plot(lm1, 2)
```
:::

::: {.column  width="60%"}
**Gr√°fico Q-Q**

A diferencia del gr√°fico Q-Q que vimos para las pruebas t, en el eje y de este mismo gr√°fico para el ANOVA (y regresi√≥n lineal) se representan los residuos estandarizados. La interpretaci√≥n es la misma: idealmente los puntos deber√≠an ir a lo largo de la diagonal. Cuando no es as√≠, evidencia una violaci√≥n del supuesto de la normalidad.
:::
:::

## Diagn√≥sticos del ANOVA {.smaller visibility="uncounted"}

::: {.columns .v-center-container}
::: {.column  width="40%"}
```{r echo=F, eval=T, error=T, fig.height=6, fig.width=6}
plot(lm1, 3)
```
:::

::: {.column  width="60%"}
**Ra√≠z cuadrada de los residuos estandarizados vs. Valores ajustados**

Similar al primer diagn√≥stico, en el caso del ANOVA, nos da una idea de posibles departuras de la homogeneidad de las varianzas. La l√≠nea roja idealmente deber√≠a ser completamente horizontal.
:::
:::

## Diagn√≥sticos del ANOVA {.smaller visibility="uncounted"}

::: {.columns .v-center-container}
::: {.column  width="40%"}
```{r echo=F, eval=T, error=T, fig.height=6, fig.width=6}
plot(lm1, 5)
```
:::

::: {.column  width="60%"}
**Residuos vs. Apalancamiento**

Aquellos puntos que est√©n etiquetados con n√∫meros son mostrados como posibles outliers bajo dos criterios:

-   Est√°n por fuera de los l√≠mites de la regla del rango intercuart√≠lico (IQR), y

-   Marcados como outliers con influencia de apalancamiento mediante la prueba de Cook (distancia de Cook).

El segundo criterio es un argumento s√≥lido para remover outliers.
:::
:::

## Pruebas formales de los supuestos del ANOVA

**Normalidad de los residuos**

```{r echo=T, eval=T, error=T, fig.height=6, fig.width=6}
residuos <- lm1$residuals
shapiro.test(residuos)
```

## Pruebas formales de los supuestos del ANOVA

**Homogeneidad de las varianzas** con `{car}`

```{r echo=T, eval=F, error=T, fig.height=6, fig.width=6}
install.packages("car")
library(car)
leveneTest(lm1)
```

```{r echo=F, eval=T, error=T, fig.height=6, fig.width=6}
library(car)
leveneTest(lm1)
```

## Comparaciones m√∫ltiples {.smaller}

::: incremental

-   Las comparaciones m√∫ltiples m√°s usadas son:

    -   HSD Tukey (*Honestly significant difference*): lleva a cabo todos los pares de comparaciones posibles entre los niveles de un factor.

    -   Prueba de Dunnett: Compara los niveles √∫nicamente con respecto al nivel control dentro del factor.

-   Son conocidas tambi√©n como pruebas *post-hoc*.

-   En R, una manera de realizar comparaciones m√∫ltiples es mediante los paquetes `{emmeans}` y `{multcomp}` (este √∫ltimo depende de `{multcompView}`, as√≠ que no olvides instalarlo tambi√©n).
:::

## HSD Tukey {.smaller}

. . .

Calculamos las medias marginales a partir del modelo

```{r echo=T, eval=F, error=T, fig.align = 'center', fig.width=6}
#| code-line-numbers: "1|2|3"
library(emmeans)
ph1 <- emmeans(lm1, specs = "Treatment")
summary(ph1)
```

## HSD Tukey {.smaller autoanimate="true"}

Calculamos las medias marginales a partir del modelo

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
library(emmeans)
ph1 <- emmeans(lm1, specs = "Treatment")
summary(ph1)
```

. . .

-   Ahora podemos calcular las comparaciones por pares de HSD Tukey

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
tukey_comp <- contrast(ph1, specs = "Treatment", method = "tukey")
tukey_comp
```

## Prueba de Dunnett {.smaller .scrollable}

. . .

-   Para Dunnett, es importante el establecer el grupo control

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
dunnett_comp <- contrast(ph1, specs = "Treatment", method = "dunnett", ref = "none")
dunnett_comp
```


## Agrupaci√≥n de comparaciones m√∫ltiples {.smaller}

-   Finalmente, otra tabla de resumen de las comparaciones m√∫ltiples es la de agrupar las medias aritm√©ticas marginales con n√∫meros (o letras) de acuerdo a si estas son estad√≠sticamente distintas o no entre si. Para ello podemos usar el paquete `multcomp`:

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
# multcomp necesita un paquete extra llamada multcompView
# No olvides instalar multcompView antes de correr este c√≥digo
library(multcomp)
medias_marginales <- cld(ph1)
medias_marginales
```


## Antes de continuar {.smaller}

::: incremental
-   En este punto, antes de continuar hagamos uso nuevamente del paquete `flextable` para exportar nuestras tablas a Word.
:::

. . .

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
library(flextable)
tabla_anova <- anova1 |> 
  report_table() |> 
  format_table() |>
  flextable()
tabla_tukey <- colformat_double(flextable(as.data.frame(tukey_comp)), 
                                digits = 3, j = c(2, 3, 6)) |>
  autofit()
tabla_dunnett <- colformat_double(flextable(as.data.frame(dunnett_comp)), 
                                  digits = 3, j = c(2, 3, 6)) |>
  autofit()
tabla_marginal <- colformat_double(flextable(medias_marginales), 
                                   digits = 3, j = c(2, 3, 5, 6)) |>
  autofit()
```

. . .

```{r echo=F, eval=T, error=T, fig.align = 'center', fig.width=6}
tabla_anova
```

## Antes de continuar {.smaller visibility="uncounted"}

```{r echo=F, eval=T, error=T, fig.align = 'center', fig.width=6}
tabla_tukey 
tabla_dunnett
tabla_marginal
```

## Antes de continuar {visibility="uncounted"}

```{r echo=T, eval=F, error=T, fig.align = 'center', fig.width=6}
save_as_docx("Tabla Anova" = tabla_anova, "Tabla Tukey" = tabla_tukey, "Tabla Dunnett" = tabla_dunnett,
             "Tabla Medias Marginales Esperadas" = tabla_marginal,
             path = "C:/Users/mmore/Documents/cursos_2024/biohack/biohack-modulo5/tablas.docx")
```

![](images/anova_tabla.png){fig-align="center"}

## Gr√°ficos de comparaciones m√∫ltiples {.smaller}

Librer√≠a `{ggstatsplot}`

::: columns
::: {.column width="50%"}
```{r echo=T, eval=F, error=T, fig.align = 'center'}
library(ggstatsplot)
ggbetweenstats(data = FruitFlies,
               x = Treatment,
               y = Longevity,
               pairwise.comparisons = T,
               pairwise.display = "significant",
               p.adjust.method = "none")
```
:::

::: {.column .fragment width="50%"}
```{r echo=F, eval=T, error=T, fig.align = 'center'}
library(ggstatsplot)
ggbetweenstats(data = FruitFlies,
               x = Treatment,
               y = Longevity,
               pairwise.comparisons = T,
               pairwise.display = "significant",
               p.adjust.method = "none")
```

:::
:::

## ANOVA de un dise√±o desbalanceado {.smaller}

::: incremental
-   Es com√∫n perder observaciones durante un experimento. Esto no representa problemas para llevar a cabo un ANOVA siempre y cuando:

    -   El porcentaje de observaciones perdidas no es alto (al menos 3 observaciones por tratamiento).
    
    -   La perdida de observaciones no conlleve a desviaciones de los supuestos del m√©todo.

-   La mayor√≠a de m√©todos estad√≠sticos requieren ser corregidos ante observaciones perdidas para poder tener la certeza de que los estimados que obtenemos no sean sesgados.

-   Sin adentrarnos en mayor detalle, uno de los componentes de la tabla de ANOVA es la suma de cuadrados. Existen tres tipos de suma de cuadrados: I, II y III.

-   En breve, las sumas II y III se aconseja sean usadas cuando existen interacciones en el ANOVA.

-   En R, la funci√≥n `aov` calcula la suma de cuadrados tipo I. Este tipo de suma no es conveniente ante la presencia de desbalance de los datos.

-   En cambio, **la funci√≥n `Anova` del paquete `{car}`**, usa por default el tipo II que es precisamente el recomendado usar ante la presencia de desbalance.

:::

## Ecuaci√≥n de un ANOVA {.smaller}

::: incremental

-   Es bastante com√∫n el presentar la ecuaci√≥n del ANOVA (y otros modelos estad√≠sticos) ya sea en reportes, tesis, art√≠culos, etc.

-   Podemos extraerla de una variedad de modelos en R con la ayuda de `{equatiomatic}`

:::

. . .

```{r echo=T, eval=F, error=T, fig.align = 'center'}
install.packages("equatiomatic")
library(equatiomatic)
extract_eq(anova1)
```

Esto nos devuelve en la consola lo siguiente:

`\operatorname{Longevity} = \beta_{1}(\operatorname{Treatment}) + \beta_{2}() + \epsilon`

. . .

Esta l√≠nea est√° dada en c√≥digo $\LaTeX$, que renderizada se ve as√≠

```{r echo=F, eval=T, error=T, fig.align = 'center'}
library(equatiomatic)
extract_eq(anova1)
```

. . .

En el **M√≥dulo 8** aprenderemos como renderizar directamente este tipo de expresiones desde RStudio, usando Quarto

## Prueba de Kruskal-Wallis {.smaller}

::: incremental

-   La prueba de Kruskal-Wallis es la alternativa no param√©trica al ANOVA de una v√≠a.

-   Puede extenderse al ANOVA de m√∫ltiples v√≠as reorganizando el dise√±o experimental.

-   Similar a las pruebas de Wilcoxon, se basa en encontrar diferencias de las medianas en lugar de las medias y su poder es menor.

-   Para ilustrar este ejemplo, supongamos que la variable `Longevity` no cumpli√≥ los supuestos del ANOVA y veamos si existen diferencias con respecto al tratamiento `Treatment` usando Kruskal-Wallis.

:::

. . .

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
kruskal.test(Longevity ~ Treatment, data = FruitFlies)
```

## Comparaciones m√∫ltiples con Kruskal-Wallis {.smaller}

::: incremental

-   Con KW tambi√©n podemos hacer comparaciones m√∫ltiples.

-   En R base contamos con la funci√≥n `pairwise.wilcox.test` que lleva a cabo comparaciones por pares mediante el m√©todo de Wilcoxon.

:::

. . .

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
pairwise.wilcox.test(FruitFlies$Longevity, FruitFlies$Treatment, p.adjust.method = "BH")
```

## ANOVA de m√∫ltiples v√≠as {.smaller}

::: incremental

-   El ANOVA puede extenderse para analizar dos o m√°s factores a la vez (2 factores: ANOVA de 2 v√≠as; 3 factores: ANOVA de 3 v√≠as...)

-   Se lo usa primordialmente para entender posibles interacciones entre los factores.

-   Es recomendable dise√±ar experimentos hasta m√°ximo 3 factores:

    -   Costos de investigaci√≥n
    
    -   A m√°s factores, es m√°s dif√≠cil interpretar interacciones
    
    -   A m√°s factores, es m√°s f√°cil caer en interacciones sin sentido
    
-   ANOVA de dos v√≠as

    -   ANOVA aditivo de dos v√≠as (modelo de los efectos principales)
    
    -   ANOVA no aditivo de dos v√≠as (modelo con interacci√≥n)

:::

## ANOVA aditivo de dos v√≠as {.smaller}

::: incremental

-   Tabla de datos de los "dedos fren√©ticos" (*frantic fingers*) de `{Stat2Data}`

-   Estos datos corresponden a un art√≠culo publicado por [Scott y Chen](https://jpet.aspetjournals.org/content/82/1/89){target="_blank"} donde compararon los efectos de la cafe√≠na y teobromina sobre 4 personas a quienes se les suministr√≥ una tableta conteniendo estos compuestos o un placebo (grupo control).

-   Dos horas despu√©s de la ingesti√≥n, les fue dada la tarea de tocar sus dedos de maneras predeterminadas. El n√∫mero de veces en que lo hicieron, fue registrada a lo largo de varios d√≠as.
:::

. . .

```{r echo=T, eval=T, error=T, fig.align = 'center', fig.width=6}
data(FranticFingers)
FranticFingers
```

## ANOVA aditivo de dos v√≠as {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}

-   Contamos con dos variables categ√≥ricas: `Subj` y `Drug` que refieren al individuo y el compuesto suministrado.

```{r echo=T, eval=F, error=T, fig.align = 'center'}
FranticFingers |>
  ggplot(aes(x = Drug,  y = Rate)) +
  geom_boxplot() +
  geom_point(aes(color = Subj), size = 3) +
  theme_bw()
```
:::

::: {.column width="50%"}
```{r echo=F, eval=T, error=T, fig.align = 'center'}
FranticFingers |>
  ggplot(aes(x = Drug,  y = Rate)) +
  geom_boxplot() +
  geom_point(aes(color = Subj), size = 3) +
  theme_bw()
```

:::
:::


::: columns
::: {.column width="50%" .fragment}

-   El factor `Subj` no es de inter√©s, pero notamos como existen diferencias entre sus niveles.

-   Si analizaramos los datos como un ANOVA de una v√≠a podr√≠amos cometer un error

:::

::: {.column width="50%" .fragment}

```{r echo=T, eval=T, error=T, fig.align = 'center'}
lm2 <- lm(Rate ~ Drug, data = FranticFingers)
Anova(lm2)
```
:::
:::


## ANOVA aditivo de dos v√≠as {.smaller visibility="uncounted"}


::: columns
::: {.column width="50%"}

-   El efecto de `Drug` puede estar siendo enmascarado por el factor `Subj`.

-   Una manera de corregir, es a√±adir `Subj` a manera de un efecto principal en el modelo

```{r echo=T, eval=T, error=T, fig.align = 'center'}
lm3 <- lm(Rate ~ Drug + Subj, data = FranticFingers)
Anova(lm3)
```


:::

::: {.column width="50%" .fragment}

-   Chequeamos supuestos

```{r echo=T, eval=T, error=T, fig.align = 'center'}
# Homogeneidad de las varianzas
leveneTest(Rate ~ Drug, data = FranticFingers)
# Normalidad de los residuos
residuos <- lm3$residuals
shapiro.test(residuos)
```
:::
:::

## ANOVA aditivo de dos v√≠as {.smaller visibility="uncounted"}

::: incremental

-   En general, en una ANOVA aditivo de dos v√≠as, el segundo factor usado como efecto principal cumple un rol de "correcci√≥n" del an√°lisis.

-   Este factor puede referirse a una caracter√≠stica en com√∫n para las observaciones (en el ejemplo dado, el individuo que llevo a cabo el ejercicio).

-   Este factor por lo general no se lo interpreta ya que no agrega informaci√≥n importante a la pregunta de investigaci√≥n inicial.

-   Por lo tanto, comparaciones m√∫ltiples se llevan a cabo de manera similar a como vimos para el ANOVA de una v√≠a.
:::

## ANOVA **no** aditivo de dos v√≠as {.smaller}

-   Tabla `PigFeed` de `{Stat2Data}` sobre los effectos de aditivos en comida para cerdos sobre su engorde.

-   Un investigador midi√≥ la ganancia de peso en cerdos al someterlos a una dieta que inclu√≠a vitamina B12 y antibi√≥ticos.

-   Variables: `WgtGain` ganancia de peso (libras), `Antibiotic` si o no, y `B12` si o no.

```{r echo=T, eval=T, error=T, fig.align = 'center'}
data("PigFeed")
PigFeed
```


## ANOVA **no** aditivo de dos v√≠as {.smaller visibility="uncounted"}

::: columns
::: {.column width="50%"}

-   Para un ANOVA no aditivo, podemos usar esta sintaxis

```{r echo=T, eval=T, error=T, fig.align = 'center'}
lm4 <- lm(WgtGain ~ B12 * Antibiotic, data = PigFeed)
alm4 <- Anova(lm4)
alm4
report(alm4)
```
:::

::: {.column width="50%" .fragment}

-   Checamos supuestos

```{r echo=T, eval=T, error=T, fig.align = 'center'}
# Homogeneidad de las varianzas
leveneTest(lm4)
# Normalidad de los residuos
residuos <- lm4$residuals
shapiro.test(residuos)
```

:::
:::


## ANOVA **no** aditivo de dos v√≠as {.smaller visibility="uncounted"}

**Gr√°ficos de interacci√≥n**

-   Podemos usar `emmip` de `{emmeans}`

. . .

::: columns
::: {.column width="50%"}

```{r echo=T, eval=T, error=T, fig.align = 'center'}
emmip(lm4, Antibiotic ~ B12)
```
:::

::: {.column width="50%"}

```{r echo=T, eval=T, error=T, fig.align = 'center'}
emmip(lm4, B12 ~ Antibiotic)
```

:::
:::

## ANOVA **no** aditivo de dos v√≠as {.smaller visibility="uncounted"}

**Comparaciones m√∫ltiples en ANOVA no aditivo**

```{r echo=T, eval=T, error=T, fig.align = 'center'}
emlm4 <- emmeans(lm4, specs = "Antibiotic", by = "B12", method = "tukey")
cld(emlm4)
```


## Antes de terminar...

![](images/tests.jpg){fig-align="center" width=900}

:::footer
[statstest.com](https://www.statstest.com/){target="_blank"}
:::

## {}

::: columns
::: {.column width="60%"}

::: {.title data-id="title"}
Fin del m√≥dulo 5
:::

::: {.callout-tip}
## Cr√©ditos de fotos

Foto final por <a href="https://unsplash.com/@cleipelt?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Chris Leipelt</a> en <a href="https://unsplash.com/photos/rock-formation-on-the-sea-bOHg3NmtWgE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
  
  

Resto de fotos: Varias fuentes
:::

:::

::: {.column width="37.5%"}

![](images/icon1_nb.png){style="margin-left:120px"}


:::
:::

![](images/chris-leipelt-bOHg3NmtWgE-unsplash.jpg){.image-right}
